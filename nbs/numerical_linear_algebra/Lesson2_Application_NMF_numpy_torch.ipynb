{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non negative Matrix Factorization from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook, we will implement the **Non negative Matrix Factorization** From scratch. I will try to compare two implementation. The first one uses the vanilla **numpy**. For the second,we will accelerate the process by calling all the computations on `gpu`s using **pytorch**.\n",
    "\n",
    "<img src=\"images/nmf_doc.png\" alt=\"NMF on documents\" style=\"width: 80%\"/>\n",
    "\n",
    "[NMF tutorial](../documents/NMF_tutorial_ICME-2014.pdf)\n",
    "\n",
    "\n",
    "\n",
    "## Group news\n",
    "\n",
    "In this notebook, we will apply the **PCA** and **NMF** to compute those factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import scipy.linalg as linalg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of categories and things to remove\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "newsgroup_train = fetch_20newsgroups(subset='train', categories = categories, remove = remove)\n",
    "newsgroup_test = fetch_20newsgroups(subset='test', categories= categories, remove =  remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   categories are ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "   Size of the documents (2034,)\n",
      "   Single example Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n"
     ]
    }
   ],
   "source": [
    "#printing information about newgropus\n",
    "print(\"   categories are\",newsgroup_train.target_names)\n",
    "print(\"   Size of the documents\", newsgroup_train.filenames.shape)\n",
    "print(\"   Single example\",newsgroup_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF matrix \n",
    "\n",
    "We will compute the **TF-IDF** matrix for each document. This matrix will serve a the main matrix $\\mathbf{V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectors_tf_idf=vectorizer.fit_transform(newsgroup_train.data).todense()  # We use todense since the method return a scipy  sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Shape of the vectors is: (2034, 26576)\n"
     ]
    }
   ],
   "source": [
    "print(\"    Shape of the vectors is:\",vectors_tf_idf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF with numpy\n",
    "\n",
    "We will use **SGD** to compute the decompostion. The method used by *sklearn* minimize the following function:\n",
    "\n",
    "$$\n",
    "E(W,H) = \\Vert R - W*H\\Vert_2 + \\lambda \\big(  \\alpha ( \\Vert H \\Vert_1 + \\Vert W\\Vert_1 ) + (1-\\alpha)( \\Vert H \\Vert_{fro} + \\Vert W\\Vert_{fro}) \\big)\n",
    "$$\n",
    "\n",
    "For the objectif function we will only consider the **Frobenieus** Norm for simplification. This choice conform with the value  of $\\alpha=0$. in the previous objectif function.\n",
    "\n",
    "$$\n",
    "E(W,H) = \\Vert R - W*H\\Vert_2 + \\lambda \\big(  \\Vert H \\Vert_{fro} + \\Vert W\\Vert_{fro} \\big)\n",
    "$$\n",
    "\n",
    "In order to compute the matrices $W$ and $H$. We will start with random matrices and iteratively apply the **SGD** to minimize the objectif function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shapes\n",
    "m,n = vectors_tf_idf.shape\n",
    "d=5  # num topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisations\n",
    "W = np.random.randn(m,d)\n",
    "H = np.random.randn(d,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Initial loss is : 17114.88842\n"
     ]
    }
   ],
   "source": [
    "#Now let's  define our loss\n",
    "R = vectors_tf_idf   # simple name for the tf_idf\n",
    "def loss(W,H,lam=1):\n",
    "    \"\"\"\n",
    "    Compute the loss of the matrix W,H\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(R - W@H) + lam* ( np.linalg.norm(W,ord='fro')+ np.linalg.norm(H,ord='fro'))\n",
    "\n",
    "print(\"    Initial loss is : {:.5f}\".format(loss(W,H)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient according to each matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    (2034, 5)\n",
      "    (5, 26576)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anass/anaconda3/envs/fastai/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in greater\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def penalty(W,tol=1e-15):\n",
    "    \"\"\"\n",
    "    Compute the error according the frobenius norm\n",
    "    \"\"\"\n",
    "    return np.where(W>tol, 0 , np.min(W-tol,0))\n",
    "def gradients(W,H,lam=1):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss function \n",
    "    return dW, dH\n",
    "    \"\"\"\n",
    "    \n",
    "    #compute the difference\n",
    "    err= W@H - R\n",
    "    \n",
    "    return err@H.T+ lam*penalty(W), W.T@err + lam*penalty(H)\n",
    "\n",
    "dW, dH = gradients(W,H)\n",
    "print(\"   \",dW.shape)\n",
    "print(\"   \",dH.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Now that we computed the loss function and the gradients, we will iteratively apply the gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rate = 0.1\n",
    "def update(W,H,lr=lr_rate):\n",
    "    \"\"\"\n",
    "    Function to performe one iteration of SGD\n",
    "    \"\"\"\n",
    "    #compute the gradients\n",
    "    dW, dH = gradients(W,H)\n",
    "    \n",
    "    W -= lr * dW\n",
    "    H -= lr * dH\n",
    "    \n",
    "    return W,H\n",
    "    \n",
    "def report(W,H):\n",
    "    \"\"\"\n",
    "    Function to plot the statistics about the matrix\n",
    "    \"\"\"\n",
    "    loss = np.linalg.norm(R- W@H)\n",
    "    print(\"    loss: {:.4f}, min values = {:.2f},{:.2f}, negative values = {:d}, {:d} \".format(loss, W.min(),H.min(), np.sum(W<0),np.sum(H<0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, H = np.abs(np.random.normal(scale=0.01, size=(m,d))), np.abs(np.random.normal(scale=0.01, size=(d,n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    loss: 44.4267, min values = 0.00,0.00, negative values = 0, 0 \n"
     ]
    }
   ],
   "source": [
    "report(W,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    loss: 44.5484, min values = -0.01,-0.02, negative values = 557, 1918 \n"
     ]
    }
   ],
   "source": [
    "W, H = update(W, H)\n",
    "report(W, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    loss: 43.7546, min values = -0.22,-0.24, negative values = 1744, 51527 \n",
      "    loss: 43.7458, min values = -0.22,-0.22, negative values = 1697, 53328 \n",
      "    loss: 43.7398, min values = -0.20,-0.23, negative values = 1579, 53645 \n",
      "    loss: 43.7356, min values = -0.19,-0.26, negative values = 1588, 52952 \n",
      "    loss: 43.7319, min values = -0.17,-0.29, negative values = 1514, 52448 \n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    W, H = update(W,H)\n",
    "    \n",
    "    if i % 10== 0:\n",
    "        report(W,H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the values by showing the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words=8\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "def show_topics(a):\n",
    "    \"\"\"\n",
    "    Function to return the num_top_words in a topic\n",
    "    \"\"\"\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['space don people nasa think like know just',\n",
       " 'thanks graphics files image file program windows format',\n",
       " 'objective morality values just moral science graphics think',\n",
       " 'space ico bobbe tek bronx beauchaine vice manhattan',\n",
       " 'god jesus bible people believe say atheism does']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good result. But the implementation is slow. We will accelerate this implementation using `pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "\n",
    "[PyTorch](http://pytorch.org/) is a Python framework for tensors and dynamic neural networks with GPU acceleration.  Many of the core contributors work on Facebook's AI team.  In many ways, it is similar to Numpy, only with the increased parallelization of using a GPU.\n",
    "\n",
    "From the [PyTorch documentation](http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html):\n",
    "\n",
    "<img src=\"images/what_is_pytorch.png\" alt=\"pytorch\" style=\"width: 80%\"/>\n",
    "\n",
    "**Further learning**: If you are curious to learn what *dynamic* neural networks are, you may want to watch [this talk](https://www.youtube.com/watch?v=Z15cBAuY7Sc) by Soumith Chintala, Facebook AI researcher and core PyTorch contributor.\n",
    "\n",
    "If you want to learn more PyTorch, you can try this [tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) or this [learning by examples](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "import torch\n",
    "import torch.cuda as tc\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying the tf_ifd to cuda\n",
    "\n",
    "t_vectors = torch.Tensor(R.astype(np.float32)).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalty(W,tol=1e-15):\n",
    "    \"\"\"\n",
    "    Compute the error according the frobenius norm\n",
    "    \"\"\"\n",
    "    return (W<tol).type(tc.FloatTensor)*torch.clamp(W - tol, max=0.)\n",
    "def gradients(W,H,lam=1):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss function \n",
    "    return dW, dH\n",
    "    \"\"\"\n",
    "    \n",
    "    #compute the difference\n",
    "    err= W.mm(H)  - t_vectors\n",
    "    \n",
    "    return err.mm(H.t())+ lam*penalty(W), W.t().mm(err) + lam*penalty(H)\n",
    "\n",
    "\n",
    "def update(W,H,lr=lr_rate):\n",
    "    \"\"\"\n",
    "    Function to performe one iteration of SGD\n",
    "    \"\"\"\n",
    "    #compute the gradients\n",
    "    dW, dH = gradients(W,H)\n",
    "    W.sub_(lr*dW)\n",
    "    H.sub_(lr*dH)\n",
    "    \n",
    "def report(W,H):\n",
    "    \"\"\"\n",
    "    Function to plot the statistics about the matrix\n",
    "    \"\"\"\n",
    "    loss = (t_vectors- W.mm(H)).norm(2)\n",
    "    print(\"    loss: {:.4f}, min values = {:.2f},{:.2f}, negative values = {:d}, {:d} \".format(loss, W.min(),H.min(), (W<0).sum(),(H<0).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialistaion in the gpu\n",
    "t_W = tc.FloatTensor(m,d)\n",
    "t_H = tc.FloatTensor(d,n)\n",
    "t_W.normal_(std=0.01).abs_(); \n",
    "t_H.normal_(std=0.01).abs_();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    loss: 44.3653, min values = -0.01,-0.00, negative values = 1408, 3348 \n",
      "    loss: 43.7283, min values = -0.10,-0.16, negative values = 3657, 33919 \n",
      "    loss: 43.7037, min values = -0.13,-0.10, negative values = 3532, 36118 \n",
      "    loss: 43.7003, min values = -0.09,-0.09, negative values = 3353, 34852 \n",
      "    loss: 43.7000, min values = -0.06,-0.08, negative values = 3608, 34870 \n",
      "    loss: 43.7000, min values = -0.05,-0.07, negative values = 3684, 36958 \n",
      "    loss: 43.7000, min values = -0.05,-0.07, negative values = 3692, 39172 \n",
      "    loss: 43.7000, min values = -0.05,-0.07, negative values = 3688, 40815 \n",
      "    loss: 43.7000, min values = -0.05,-0.07, negative values = 3698, 42310 \n",
      "    loss: 43.7001, min values = -0.05,-0.07, negative values = 3702, 43702 \n"
     ]
    }
   ],
   "source": [
    "lr = 0.05\n",
    "for i in range(1000): \n",
    "    update(t_W,t_H,lr)\n",
    "    if i % 100 == 0: \n",
    "        report(t_W,t_H)\n",
    "        lr *= 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['space nasa shuttle launch orbit moon lunar earth',\n",
       " 'think don people just objective like morality moral',\n",
       " 'ico bobbe tek bronx beauchaine manhattan sank queens',\n",
       " 'thanks graphics files image file know program windows',\n",
       " 'god jesus bible believe christian does atheism belief']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(t_H.cpu().numpy())  # we realized several transers (gpu---> cpu -- numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
